# Данные

Данные для исследования взяты из соревнования на платформе _Kaggle_: https://www.kaggle.com/c/dog-breed-identification/overview/description. Обучение во всех экспериментах проводилось дважды: на исходном датасете соревнования, и на искусственно сокращенном - для того, чтобы сравнить ухудшение качества классификации при обучении на выборках разного размера. Обучающая выборка полного датасета содержит около 8К изображений, принадлежащих 120 классам. В среднем в каждом классе 64 изображения; самый многочисленный класс содержит 94 картинки, а самый малочисленный - 50.
В сокращенном датасете в каждом классе в два раза меньше картинок. Тестирование осуществлялось на закрытой тестовой выборке соревнования, результат рассчитывался по метрике _Multi Class Log Loss_ путем сабмита. 

**https://stats.stackexchange.com/questions/113301/multi-class-logarithmic-loss-function-per-class**
<div align="center">
  <img src="" />
</div></br>

# Сверточные нейронные сети

В данном исследовании использовались архитектуры сверточных нейросетей `VGG-19`, `ResNet-152`, `DenseNet-161`, `EfficientNet-B6`. Модели предобучены на датасете `ImageNet`. Параметры обучения для каждой из сетей:

- Функция потерь: _CrossEntropyLoss_
- Скорость обучения: 0.00005
- Количество эпох: 10
- Оптимизатор: _AdamW_


## Обучение моделей

Результаты обучения моделей представлены в таблице:

| Model           | Params   |   Score (full)  | Score (short) | delta   |
| :-------------: |:--------:| :-------------: |:-------------:|:-------:|
| VGG-19          |   144M   |     1.01253     |    1.47443    | 0.4619  |
| ResNet-152      |    60M   |     1.18338     |    1.12304    | -0.0603 |
| DenseNet-161    |    26M   |     1.01522     |    1.46973    | 0.4545  |
| EfficientNet-B6 |    43M   |     0.84965     |    0.86976    | 0.0201  |

Лучший результат показала модель `EfficientNet-B6` - не только качество классификации лучше, но и разница между обучением на сокращенном и полном датасетах меньше, чем у остальных 
моделей, что говорит о том, что деградация результата с уменьшением обучающей выборки меньше.

## Дифференцированное обучение моделей

В этом эксперименте слои нейросети обучались с разной скоростью - у самых глубоких слоев скорость обучения значительно меньше (от 100 до 1000 раз). Скорость обучения постепенно нарастает от глубоких слоев к классификатору. Результаты обучения представлены в таблице:

| Model           | Params   |   Score (full)  | Score (short) | delta   |
| :-------------: |:--------:| :-------------: |:-------------:|:-------:|
| VGG-19          |   144M   |     0.86622     |    1.17212    | 0.3059  |
| ResNet-152      |    60M   |     0.72972     |    0.80292    | 0.0732  |
| DenseNet-161    |    26M   |     0.73347     |    1.04323    | 0.3098  |
| EfficientNet-B6 |    43M   |     0.89958     |    0.93136    | 0.0318  |

В целом, полученное качество выше для всех моделей по сравнению с обучением всех слоев, как на полном датасете, так и на сокращенном. Разница классификации при обучении на выборках разного размера по сравнению с предыдущим
экспериментом получилась больше только для модели `EfficientNet-B6`, но незначительно.


## Обучение моделей с использованием аугментаций


| Model           | Params   |   Score (full)  | Score (short) | delta   |
| :-------------: |:--------:| :-------------: |:-------------:|:-------:|
| VGG-19          |   144M   |    0.90778      |   1.39809     | 0.4903  |
| ResNet-152      |    60M   |    1.01417      |   1.11357     | 0.0994  |
| DenseNet-161    |    26M   |    0.99121      |   1.39809     | 0.4067  |
| EfficientNet-B6 |    43M   |    0.71773      |   0.75062     | 0.0329  |
